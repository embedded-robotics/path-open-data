{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5713a7ef",
   "metadata": {},
   "source": [
    "https://internvl.github.io \\\n",
    "https://github.com/OpenGVLab/InternVL \\\n",
    "https://huggingface.co/OpenGVLab/InternVL3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38442da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744558f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA_VISIBLE_DEVICES to expose only device 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to expose devices 0, 1, 2, and 3\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adeac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/path-opendata/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c63fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8533f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e111b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac16742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"OpenGVLab/InternVL3-8B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c23083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL3-8B\", trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee66916",
   "metadata": {},
   "source": [
    "#### Single Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5accd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('../DeepSeek-VL2/images/visual_grounding_1.jpeg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06581861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image shows two giraffes standing in a field with green grass and trees in the background, under a clear blue sky.\n"
     ]
    }
   ],
   "source": [
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39d87478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image in detail.\n",
      "Assistant: The image features two giraffes standing on a grassy plain. The taller giraffe is prominently positioned in the foreground, with distinctive brown and white patterned fur, while the second giraffe appears slightly behind it. The scene is set against a backdrop of trees, indicating a natural habitat or wildlife reserve. The sky is clear and blue, suggesting a bright, sunny day. The overall setting gives off a serene and wide-open savanna atmosphere.\n"
     ]
    }
   ],
   "source": [
    "# single-image multi-round conversation (单图多轮对话)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f983145",
   "metadata": {},
   "source": [
    "#### Multi-Image Multi-Round Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3bc2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Image-1: <image>\n",
      "Image-2: <image>\n",
      "Describe the two images in detail.\n",
      "Assistant: Image-1: In the image, two giraffes stand in a verdant field with a backdrop of trees and a clear blue sky. The giraffe on the left is taller and closer to the camera, showcasing its distinctive brown and white spotted pattern and long neck. The giraffe on the right is slightly shorter and positioned behind the first, also displaying the characteristic spotted pattern. Their long legs extend down to the lush green grass, and they appear to be in a natural, open environment. In the distant background, other giraffes can be faintly seen, suggesting this might be a wildlife reserve or a protected area.\n",
      "\n",
      "Image-2: In the image, two cartoon characters, labeled \"我\" (me) and \"导\" (director), are engaged in a conversation. The character labeled \"我\" appears calm and is smiling, while the character labeled \"导\" looks visibly annoyed, as indicated by the red lines on his face and the flames around his head. A speech bubble above them contains Chinese characters \"你看 又急\" (You see, he's in a hurry again), suggesting that the director is frustrated with the situation. The overall scene depicts a common workplace interaction where a director is annoyed with someone else.\n",
      "User: What are the similarities and differences between these two images.\n",
      "Assistant: The similarities between the two images include their use of speech bubbles to convey dialogue and the presence of characters engaged in interaction. Both images are humorous, using exaggerated facial expressions and common symbols (such as flames for anger) to emphasize the emotions being expressed.\n",
      "\n",
      "The differences between the two images are quite pronounced. The first image is a photograph of two giraffes in a natural setting, which conveys a serene and peaceful scene typical of wildlife photography. The second image, on the other hand, is a cartoon illustration depicting a typical office scenario with two characters—one appears to be frustrated while the other is smiling calmly. This image focuses on human emotions and workplace dynamics rather than nature. The presence of a speech bubble and the exaggerated, stylized art of the office setting contrast sharply with the realistic depiction of giraffes in their natural habitat in the first image.\n"
     ]
    }
   ],
   "source": [
    "# multi-image multi-round conversation, separate images (多图多轮对话，独立图像)\n",
    "pixel_values1 = load_image('../DeepSeek-VL2/images/visual_grounding_1.jpeg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('../DeepSeek-VL2/images/visual_grounding_2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20f755",
   "metadata": {},
   "source": [
    "#### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dcbc0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Describe the image in detail.\n",
      "Assistant: The image features two giraffes standing in a grassy field. These giraffes have tall, slender necks and distinct, patchy brown and white coats, which are characteristic of their species. They are positioned against a backdrop of lush, green grass and a line of trees in the distance. The sky is clear and blue, indicating what seems to be a sunny day. In the background, some other animals can also be faintly visible, adding to the natural setting of the scene. The lighting suggests it might be early morning or late afternoon, creating long shadows on the ground.\n",
      "User: <image>\n",
      "Describe the image in detail.\n",
      "Assistant: The image is a cartoon illustration featuring two characters standing side by side, dressed in professional attire with suits and ties. \n",
      "\n",
      "1. **Person on the Left**:\n",
      "   - Wears a dark blazer, white shirt, and no tie.\n",
      "   - Smiles with a relaxed, calm expression.\n",
      "   - A speech bubble next to the head of this character reads \"我\" (wǒ), Mandarin for \"I\".\n",
      "\n",
      "2. **Person on the Right**:\n",
      "   - Appears visibly angry, with furrowed eyebrows, red cheek marks, and an open mouth indicating shouting.\n",
      "   - Expresses annoyance or frustration.\n",
      "   - A speech bubble above the character reads “你看 又急” (nǐ kàn yòu jí), which translates to \"Look, you're so impatient\" or \"Again, you're in a hurry.\"\n",
      "   - Smaller speech bubbles above the head show exasperation with frowning cloud symbols and flames, indicating rising anger.\n",
      "\n",
      "The overall tone contrasts between the calm demeanor of the first character and the irritation of the second. The scene likely represents a scenario where one person is expressing their impatience to another in a humorous way.\n"
     ]
    }
   ],
   "source": [
    "pixel_values1 = load_image('../DeepSeek-VL2/images/visual_grounding_1.jpeg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('../DeepSeek-VL2/images/visual_grounding_2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e233d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "path-opendata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
