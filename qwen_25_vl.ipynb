{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d71d051",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct \\\n",
    "https://github.com/QwenLM/Qwen2.5-VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4af1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc7e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA_VISIBLE_DEVICES to expose only device 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to expose devices 0, 1, 2, and 3\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871e1ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/path-opendata/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0d57f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ad3f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b7a0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de67dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404628a",
   "metadata": {},
   "source": [
    "#### Single Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7abf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"./DeepSeek-VL2/images/visual_grounding_2.jpg\",\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "            ],\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ceea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a82fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d60dd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=672x616>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd28df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbc7961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.feature_extraction_utils.BatchFeature"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbb85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a581ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fafe958",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0344fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a cartoon depicting two people in business attire. The person on the left, labeled \"我\" (I), appears to be smiling and pointing at the other person with a speech bubble above their head that says \"你看 又急\" (Look, you're always in a hurry). The person on the right, labeled \"导\" (Guide), looks frustrated or annoyed, with sweat drops and flames around their head, indicating anger or stress. The overall scene suggests a humorous interaction where one person is being scolded for being impatient.\n"
     ]
    }
   ],
   "source": [
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9d3fb",
   "metadata": {},
   "source": [
    "#### Batch Images Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbefa20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages1 = [\n",
    "    {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"./DeepSeek-VL2/images/visual_grounding_2.jpg\",\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "            ],\n",
    "    }\n",
    "]\n",
    "\n",
    "messages2 = [\n",
    "    {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"./DeepSeek-VL2/images/visual_grounding_1.jpeg\",\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "            ],\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [messages1, messages2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c46597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "texts = [\n",
    "    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "738f6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n', '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n']\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2054c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d5a8a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=672x616>,\n",
       " <PIL.Image.Image image mode=RGB size=728x1092>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970b9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dd675af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
       "        [151644,   8948,    198,  ..., 151644,  77091,    198]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "        ...,\n",
       "        [-1.4419, -1.4273, -1.4419,  ..., -1.2669, -1.1958, -1.0536],\n",
       "        [-1.5587, -1.5733, -1.4565,  ..., -0.8261, -1.0110, -1.0110],\n",
       "        [-1.5295, -1.3689, -1.4711,  ..., -1.2100, -1.1674, -1.1105]]), 'image_grid_thw': tensor([[ 1, 44, 48],\n",
       "        [ 1, 78, 52]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0aaa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f912e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f0aa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_texts = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff739785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\nYou are a helpful assistant.\\nuser\\nDescribe this image.\\nassistant\\nThe image is a cartoon depicting two characters in business attire. The character on the left, labeled \"我\" (I), appears to be smiling and pointing at the other character. The character on the right, labeled \"导\" (Guide), looks frustrated or angry, with flames coming out of their head and a speech bubble saying \"你看 又急\" (Look, you\\'re always in a hurry). The overall scene suggests a humorous or exaggerated interaction between the two characters, possibly highlighting a common workplace scenario where one person is perceived as being impatient or rushed by the other.',\n",
       " 'system\\nYou are a helpful assistant.\\nuser\\nDescribe this image.\\nassistant\\nThe image shows two giraffes standing on a grassy field under a clear blue sky. The giraffe in the foreground is larger and appears to be an adult, while the one in the background is smaller, likely a juvenile. Both giraffes have their characteristic long necks and spotted coats. In the background, there are trees and some open space, suggesting a natural or semi-natural environment. The lighting indicates it might be late afternoon, as the shadows are long and the light is warm.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a12276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "path-opendata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
